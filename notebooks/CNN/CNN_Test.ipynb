{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score , roc_auc_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('LLM_vs_human_test_1k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(df , model_path, model_tokenizer, max_len):\n",
    "\n",
    "    with open(model_tokenizer, \"rb\") as file:\n",
    "        tokenizer = pickle.load(file)\n",
    "\n",
    "    cnn_model = load_model(model_path)\n",
    "\n",
    "    txt_test = df['text'].astype(str).values\n",
    "    token_test = tokenizer.texts_to_sequences(txt_test)\n",
    "    pad_token_test = pad_sequences(token_test, padding='post', maxlen=max_len)\n",
    "\n",
    "    pred = cnn_model.predict(pad_token_test)\n",
    "    preds_lbl = np.where(pred > 0.5,1,0)\n",
    "\n",
    "    return preds_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cnn = cnn_model(df,model_path=r'models_and_tokenizers\\cnn\\cnn_50k.keras', model_tokenizer=r'models_and_tokenizers\\cnn\\tokenizer_cnn.pickle', max_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(lbl, pred_cnn)\n",
    "roc = roc_auc_score(lbl, pred_cnn, average='macro')\n",
    "f1 = f1_score(lbl, pred_cnn, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy : {accuracy}')\n",
    "print(f'ROC Score : {roc}')\n",
    "print(f'f1 Score : {f1}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
