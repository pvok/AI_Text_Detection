{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv(r'C:\\Users\\sagar\\OneDrive\\Desktop\\Sem 3\\Deep Learning\\Project Roberta\\to_Transformer_Train.csv')\n",
    "df_val = pd.read_csv(r'C:\\Users\\sagar\\OneDrive\\Desktop\\Sem 3\\Deep Learning\\Project Roberta\\to_Transformer_val.csv')\n",
    "\n",
    "# Reset index\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label counts:\n",
      "label\n",
      "0    10000\n",
      "1    10000\n",
      "Name: count, dtype: int64\n",
      "Validation label counts:\n",
      "label\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display label counts\n",
    "print(\"Training label counts:\")\n",
    "print(df_train['label'].value_counts())\n",
    "print(\"Validation label counts:\")\n",
    "print(df_val['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "train_texts = df_train['sentences'].tolist()\n",
    "train_labels = df_train['label'].tolist()\n",
    "val_texts = df_val['sentences'].tolist()\n",
    "val_labels = df_val['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts sample: ['[\\'the 28-year-old was a free agent after leaving blackpool, where he played 10 times last season as the tangerines were promoted to league one\\', \\' the ex-blackburn and preston man made the majority of his career appearances at scunthorpe, featuring 137 times\\', \" nolan has become crewe\\'s fourth signing of the summer\", \" jordan bowery, michael raynes and chris porter have all moved to david artell\\'s side this summer\", \\' find all the latest football transfers on our dedicated page\\', \\'\\']', \"['relationship: communication between family members essay\\\\n\\\\ntable of contents\\\\n 1', ' introduction\\\\n 2', ' relationship\\\\n 3', ' communication\\\\n 4', ' conclusion\\\\n 5', ' works cited\\\\n\\\\nintroduction\\\\n\\\\nrelations and communications with parents and relatives are one of the most important and fundamental phenomena necessary for the development of an individual', ' thus, this process of communication and interaction between several relatives can be characterized from the point of view of the relational theory of society by pierpaolo donati', ' communications with family members within the framework of a pierpaolo donati relational concept will be discussed in this paper', '\\\\n\\\\nrelationship\\\\n\\\\nseveral aspects can characterize the relationship between my family members and me', ' they include emotional closeness, a fairly strong and close bond, cohesion, loyalty in relationships, coordinated behavior, and a high degree of differentiation', ' hence, from donati’s point of view, society cannot function properly without family and family relationships (galatolo)', ' our “bonds” are described as a special and unique organization that binds and holds together people’s primary and fundamental differences', ' undoubtedly, family is one of the essential elements in a society where the individual is considered in their “full measure,” and accordingly, in each family, there are unique and individual ways and methods of interaction', '\\\\n\\\\ncommunication\\\\n\\\\ncommunication between my family and me is based on the following special and unique principles', ' our relationship is based on mutual interaction and cooperation, verbal and nonverbal methods of transmitting information, and dynamic interaction (pace)', ' primarily, the connections between members of my family and me are directly conditioned by everyday and mundane aspects that represent a sequence of individual actions', ' for example, we discuss with parents everyday topics, discuss pressing problems and prospects, and make joint plans for the future', ' within the framework of the donati theory, this relationship, in this case, is at a deeper level and connects family members without them realizing this connection', ' thus, due to such a close connection, we can easily understand the meaning of each spoken word or gesture', '\\\\n\\\\nmoreover, the family is a social institution that mediates between the individual and society at the initial stages of socialization', ' it can be noted that relationships, in principle, are a determining factor in the formation of personality, just like oxygen and food for the human body (donati 19)', ' according to the researcher of family relations, communication between family members, as a rule, is closely intertwined with opportunities for the education of personal and social virtues', ' in this regard, my parents are important and primary sources of transferring valuable and unique practical experience, knowledge, and skills', ' each of us, in turn, shares our impressions and memories and gives valuable advice and recommendations', ' in addition, the type and methods of communication change and transform over time', ' for example, over time, the attitude of my parents towards me manifests itself from “gentler” to “hard” – one of the important bases of relational communication', '\\\\n\\\\nfamily relations, being the primary producer of relational goods, remain a unique resource for the life of society, the only source of human capital', ' according to the author, the condition for this is to build relationships based on genuine love, implying the ethics of gift and reciprocity', ' we give each other sincere care and love; our communication is based on sincerity, reciprocity, and common understanding', ' for that reason, our relationship is strong and happy, and relational communication is built on unique, individual phenomena', '\\\\n\\\\nconclusion\\\\n\\\\nin brief, interaction with family members and the relational aspect of communication from donati’s point of view is one of the important principles of personality and individuality formation', ' communication between my relatives and me is based on unique and inimitable elements', ' our family has its own and individual characteristics that differ from others', ' thus, for example, i can understand and interpret every gesture, look, and word of an adult following the established “rules and standards” in the framework of relational communication', ' in addition, i can intuitively and subconsciously adopt certain actions, deeds, and phrases and demonstrate them in the future', '\\\\n\\\\nworks cited\\\\n\\\\ndonati, pierpaolo', ' “relational versus relationist sociology: a new paradigm in the social sciences', '” stan rzeczy , no', ' 12, pp', ' 15-66, 2017', ' web', '\\\\n\\\\ngalatolo, cecilia', ' “the family, the genome of society', '” family and media, 2017', ' web', '\\\\n\\\\npace, rachael', ' “what is relational communication? principals and theory explained', '” marriage, 2020', ' web', '\\\\n']\"]\n",
      "Val texts sample: ['[\\'the short version is it is a hold-over from the cold war; we were in a long, tense conflict with the soviets, with a great deal of espionage and maneuvering and posturing going on\\', \\' because of this \"communist\" rapidly became \"enemy\" in the eyes of the public, helped in no small part by a combination of propaganda and witch-hunting\\', \\' a man named mccarthy, and others in his style (before and after), made accusations of communistic thinking as a means of political attack as well\\', \\' just to make it clear, the paranoia was vast; artists (often liberal) were black-listed and rendered unable to get work, up and comers had their careers ruined due to distant associations with communist sympathizers, or were made to point the finger at other people (and so on)\\', \\' not a pleasant situation\\', \\' because of that, much of the us rhetoric became centered around our capitalism, and thus today, capitalism is considered good and communism bad\\', \\' socialistic measures and systems are often seen as a lesser form of communism, and get a bad reputation by association\\', \\' if you want a more detailed explanation of why people do not like these sorts of measures, go to rlibertarian and ask nicely - very nicely - and they will give you a run-down on why our schools of thought tend to favor capitalism; the short version is we are largely (though not entirely) under the impression that communism does not work on the large-scale, and socialistic programs essentially take more from those that can produce or earn to give to those who have done nothing to deserve it (or something of that regard), or in a manner which fails to motivate those receiving the aid to change their state\\', \\' be warned, the folks over on that subreddit may be slightly zealous owing to the average redditor being left-leaning; libertarians are rights-liberal (pro gay marriage, pro personal freedom, etc\\', \\') but fiscally conservative (prefer smaller government, less involvement, lower spending, lower taxes, etc\\', \\') - they have a reputation as being right-wing\\', \\' i think that will do for a short version\\', \\'\\']', \"['stayed here for my last night in vegas', ' the hotelcasino, although nice, overall had a cheesy feel to it', ' a lot of gold and lions', ' that said, we stayed in a recently remodeled room that had a chic, european flair to it that we just loved', ' there was a tv in our bathroom mirror! front desk was very friendly and helpful', ' definitely a good option when in vegas', ' i noticed more families here than at other hotels', '']\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preview data\n",
    "print(\"Train texts sample:\", train_texts[:2])\n",
    "print(\"Val texts sample:\", val_texts[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee1525495c64f4e83b51fcc884abe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\anaconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sagar\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392e4b002efd4270a8290c58efe6c6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323d5a6e75624561bc51e59bba5c6d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e103928d46642e5a364a25da091c6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9ced76336a4dc880797875c8358a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenization and dataset preparation\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].flatten(),\n",
    "            'attention_mask': encodings['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare dataloaders\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf0138861604ff6be82282ccca3384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\sagar\\anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model configuration\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight is trainable\n",
      "roberta.embeddings.position_embeddings.weight is trainable\n",
      "roberta.embeddings.token_type_embeddings.weight is trainable\n",
      "roberta.embeddings.LayerNorm.weight is trainable\n",
      "roberta.embeddings.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.0.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.0.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.0.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.0.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.0.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.0.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.0.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.0.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.0.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.0.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.0.output.dense.weight is trainable\n",
      "roberta.encoder.layer.0.output.dense.bias is trainable\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.1.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.1.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.1.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.1.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.1.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.1.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.1.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.1.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.1.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.1.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.1.output.dense.weight is trainable\n",
      "roberta.encoder.layer.1.output.dense.bias is trainable\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.2.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.2.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.2.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.2.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.2.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.2.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.2.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.2.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.2.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.2.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.2.output.dense.weight is trainable\n",
      "roberta.encoder.layer.2.output.dense.bias is trainable\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.3.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.3.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.3.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.3.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.3.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.3.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.3.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.3.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.3.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.3.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.3.output.dense.weight is trainable\n",
      "roberta.encoder.layer.3.output.dense.bias is trainable\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.4.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.4.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.4.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.4.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.4.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.4.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.4.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.4.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.4.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.4.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.4.output.dense.weight is trainable\n",
      "roberta.encoder.layer.4.output.dense.bias is trainable\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.5.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.5.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.5.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.5.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.5.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.5.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.5.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.5.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.5.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.5.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.5.output.dense.weight is trainable\n",
      "roberta.encoder.layer.5.output.dense.bias is trainable\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.6.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.6.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.6.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.6.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.6.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.6.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.6.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.6.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.6.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.6.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.6.output.dense.weight is trainable\n",
      "roberta.encoder.layer.6.output.dense.bias is trainable\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.7.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.7.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.7.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.7.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.7.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.7.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.7.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.7.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.7.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.7.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.7.output.dense.weight is trainable\n",
      "roberta.encoder.layer.7.output.dense.bias is trainable\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.8.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.8.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.8.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.8.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.8.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.8.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.8.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.8.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.8.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.8.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.8.output.dense.weight is trainable\n",
      "roberta.encoder.layer.8.output.dense.bias is trainable\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.9.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.9.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.9.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.9.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.9.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.9.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.9.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.9.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.9.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.9.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.9.output.dense.weight is trainable\n",
      "roberta.encoder.layer.9.output.dense.bias is trainable\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.10.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.10.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.10.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.10.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.10.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.10.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.10.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.10.output.dense.weight is trainable\n",
      "roberta.encoder.layer.10.output.dense.bias is trainable\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.11.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.11.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.11.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.11.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.11.output.dense.weight is trainable\n",
      "roberta.encoder.layer.11.output.dense.bias is trainable\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias is trainable\n",
      "classifier.dense.weight is trainable\n",
      "classifier.dense.bias is trainable\n",
      "classifier.out_proj.weight is trainable\n",
      "classifier.out_proj.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "# Print out the parameter settings to verify\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is {'trainable' if param.requires_grad else 'frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight is frozen\n",
      "roberta.embeddings.position_embeddings.weight is frozen\n",
      "roberta.embeddings.token_type_embeddings.weight is frozen\n",
      "roberta.embeddings.LayerNorm.weight is frozen\n",
      "roberta.embeddings.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.0.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.0.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.0.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.0.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.0.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.0.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.0.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.0.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.0.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.0.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.0.output.dense.weight is frozen\n",
      "roberta.encoder.layer.0.output.dense.bias is frozen\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.1.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.1.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.1.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.1.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.1.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.1.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.1.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.1.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.1.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.1.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.1.output.dense.weight is frozen\n",
      "roberta.encoder.layer.1.output.dense.bias is frozen\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.2.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.2.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.2.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.2.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.2.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.2.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.2.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.2.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.2.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.2.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.2.output.dense.weight is frozen\n",
      "roberta.encoder.layer.2.output.dense.bias is frozen\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.3.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.3.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.3.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.3.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.3.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.3.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.3.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.3.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.3.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.3.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.3.output.dense.weight is frozen\n",
      "roberta.encoder.layer.3.output.dense.bias is frozen\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.4.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.4.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.4.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.4.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.4.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.4.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.4.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.4.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.4.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.4.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.4.output.dense.weight is frozen\n",
      "roberta.encoder.layer.4.output.dense.bias is frozen\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.5.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.5.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.5.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.5.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.5.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.5.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.5.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.5.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.5.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.5.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.5.output.dense.weight is frozen\n",
      "roberta.encoder.layer.5.output.dense.bias is frozen\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.6.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.6.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.6.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.6.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.6.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.6.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.6.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.6.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.6.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.6.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.6.output.dense.weight is frozen\n",
      "roberta.encoder.layer.6.output.dense.bias is frozen\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.7.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.7.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.7.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.7.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.7.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.7.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.7.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.7.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.7.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.7.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.7.output.dense.weight is frozen\n",
      "roberta.encoder.layer.7.output.dense.bias is frozen\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.8.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.8.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.8.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.8.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.8.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.8.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.8.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.8.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.8.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.8.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.8.output.dense.weight is frozen\n",
      "roberta.encoder.layer.8.output.dense.bias is frozen\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.9.attention.self.query.weight is frozen\n",
      "roberta.encoder.layer.9.attention.self.query.bias is frozen\n",
      "roberta.encoder.layer.9.attention.self.key.weight is frozen\n",
      "roberta.encoder.layer.9.attention.self.key.bias is frozen\n",
      "roberta.encoder.layer.9.attention.self.value.weight is frozen\n",
      "roberta.encoder.layer.9.attention.self.value.bias is frozen\n",
      "roberta.encoder.layer.9.attention.output.dense.weight is frozen\n",
      "roberta.encoder.layer.9.attention.output.dense.bias is frozen\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.9.intermediate.dense.weight is frozen\n",
      "roberta.encoder.layer.9.intermediate.dense.bias is frozen\n",
      "roberta.encoder.layer.9.output.dense.weight is frozen\n",
      "roberta.encoder.layer.9.output.dense.bias is frozen\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight is frozen\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias is frozen\n",
      "roberta.encoder.layer.10.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.10.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.10.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.10.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.10.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.10.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.10.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.10.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.10.output.dense.weight is trainable\n",
      "roberta.encoder.layer.10.output.dense.bias is trainable\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.query.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.query.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.key.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.key.bias is trainable\n",
      "roberta.encoder.layer.11.attention.self.value.weight is trainable\n",
      "roberta.encoder.layer.11.attention.self.value.bias is trainable\n",
      "roberta.encoder.layer.11.attention.output.dense.weight is trainable\n",
      "roberta.encoder.layer.11.attention.output.dense.bias is trainable\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias is trainable\n",
      "roberta.encoder.layer.11.intermediate.dense.weight is trainable\n",
      "roberta.encoder.layer.11.intermediate.dense.bias is trainable\n",
      "roberta.encoder.layer.11.output.dense.weight is trainable\n",
      "roberta.encoder.layer.11.output.dense.bias is trainable\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight is trainable\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias is trainable\n",
      "classifier.dense.weight is trainable\n",
      "classifier.dense.bias is trainable\n",
      "classifier.out_proj.weight is trainable\n",
      "classifier.out_proj.bias is trainable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set the last two layers of the encoder and the classifier to be trainable\n",
    "layer_indices = [10, 11]  # The last two layers\n",
    "\n",
    "for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "    if i in layer_indices:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Make classifier layer trainable\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is {'trainable' if param.requires_grad else 'frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(\"Evaluating\")\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch['labels']\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return average_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch['labels']\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return average_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checkpoint directory\n",
    "checkpoint_dir = './model_checkpoints_Roberta_Venkat'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 2500/2500 [07:41<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0391, Train Accuracy=0.9868, Val Loss=0.2017, Val Accuracy=0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 2500/2500 [07:40<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0310, Train Accuracy=0.9890, Val Loss=0.1322, Val Accuracy=0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 2500/2500 [07:41<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0214, Train Accuracy=0.9928, Val Loss=0.0700, Val Accuracy=0.9760\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Assuming checkpoint_dir is defined\n",
    "checkpoint_dir = './model_checkpoints_Roberta_Venkat'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    train_loss, train_accuracy = train(model, train_loader, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Accuracy={train_accuracy:.4f}, Val Loss={val_loss:.4f}, Val Accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_checkpoints_finetune\\\\tokenizer_config.json',\n",
       " './model_checkpoints_finetune\\\\special_tokens_map.json',\n",
       " './model_checkpoints_finetune\\\\vocab.json',\n",
       " './model_checkpoints_finetune\\\\merges.txt',\n",
       " './model_checkpoints_finetune\\\\added_tokens.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming tokenizer is your initialized tokenizer\n",
    "tokenizer.save_pretrained('./model_checkpoints_finetune')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model_checkpoints_finetune') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
